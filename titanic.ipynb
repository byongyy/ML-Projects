{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Classification Problem\n",
    "What we have here is a dataset on the 1912 Titanic disaster. It's a binary classification problem where the labels represent a passenger's survival. This Notebook is an attempt to apply machine learning techniques to build a model capable of accurately predicting whether a passenger survives or not, based on factors such as ticket class, gender, age, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Firstly, we need to get housekeeping out of the way by importing the usual suspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "The next step is to load the data. It's in a folder called 'data' and already separated into train and test .csv's. I'll load them into pandas dataframes and take a look at how the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_train = \"./data/train_titanic.csv\"\n",
    "fname_test = \"./data/test_titanic.csv\"\n",
    "train = pd.read_csv(fname_train)\n",
    "test = pd.read_csv(fname_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 12), (418, 11))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well then.. Looks like the test data doesn't have labels included and we'll have to rely on cross validation scores to pick a model. I'll then use that final model to submit predictions on Kaggle and report the scores here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right off the bat, I can see some columns probably irrelevant in the context of machine learning, e.g. Name and Cabin. I'll get around to removing them when building the model. But first, I'd like to visualize some of the data and take a look at the distribution. Also, the column names can get abit confusing so I'll rename some of them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.rename(index=str, columns={'PassengerId': 'Id',\n",
    "                                 'Pclass': 'TicketClass',\n",
    "                                 'SibSp': 'SiblingsSpouses',\n",
    "                                 'Parch': 'ParentsChildren'})\n",
    "test = test.rename(index=str, columns={'PassengerId': 'Id',\n",
    "                                'Pclass': 'TicketClass',\n",
    "                                'SibSp': 'SiblingsSpouses',\n",
    "                                'Parch': 'ParentsChildren'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "This is a simple 0 or 1 binary value on whether the passenger survives the disaster or not. According to Google, ~68% of people on board the Titanic did not, but this dataset appears to only hold 891 passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    549\n",
       "1    342\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " That's 61% dead passengers. Not exactly a 50/50 split, but not overly skewed to make it worthy of feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "### Gender & Age\n",
    "From James Cameron's movie, we should expect women and children more likely to survive. Let's see if this is true. First, the theory that there are proportionally more women survivors than men:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x253d550c5f8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = sns.FacetGrid(data,\n",
    "                      col='Sex')\n",
    "graph.map(plt.hist, \n",
    "          'Survived',\n",
    "          bins=[0,0.5,1,1.5],\n",
    "          color='teal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, not only are there more female passengers who survived over males, but there are also far more female survivors than ones that didn't survive. The men however, suffer the exact opposite. There are over three times as many males who perished than survived. What about children? We should expect to see the same case with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x253d550cda0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = sns.FacetGrid(data, col='Survived')\n",
    "graph.map(plt.hist,\n",
    "          'Age',\n",
    "          color='teal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting on both ends of the age spectrum. There are more children and elderly survivors than casualties, while passengers aged 20-50 experience more casualties. Rightfully so, since the saying usually goes \"women, children and elderly first\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ticket Class\n",
    "Another thing worth looking into should be the breakdown of survivors by their ticket class. One might expect first class passengers were given priority on life boats, as portrayed in the movie. Let's see if that holds true in real life:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x253d7705128>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = sns.FacetGrid(data, col='Survived')\n",
    "graph.map(plt.hist,\n",
    "          'TicketClass',\n",
    "          bins=[1,1.5,2,2.5,3,3.5],\n",
    "          color='teal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking solely at the number of survivors, the count looks pretty even, with all three classes hovering around 80-120 survivors. But taking into account the number of actual passengers for each, there is a way higher proportion of first class passengers who survived than third class. In fact, there are 3 times as many third class passenger casualties than survivors, while first class passengers have more survivors than deaths. I expect this to be a significant indicator when modeling the data later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "Some tidying up should be done to help with model convergence. I'll start by removing irrelevant features like Id, Cabin, etc. I'm kind of unsure whether Embarked matters at all for the model but will keep it in for now. If the information gain from it is low, I'll drop it when training future models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['Id','Name','Ticket','Cabin'])\n",
    "test = test.drop(columns=['Id','Name','Ticket','Cabin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>TicketClass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SiblingsSpouses</th>\n",
       "      <th>ParentsChildren</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Survived, TicketClass, Sex, Age, SiblingsSpouses, ParentsChildren, Fare, Embarked]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also need to convert Sex and Embarked into numerical values. Since Sex is binary, I can simply change them to 0's and 1's. For Embarked however, I'll have to one-hot-encode the values and add the binary columns Cherbourg, Queenstown and Southampton. The same will be done on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>TicketClass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SiblingsSpouses</th>\n",
       "      <th>ParentsChildren</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cherbourg</th>\n",
       "      <th>Queenstown</th>\n",
       "      <th>Southampton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  TicketClass Sex   Age  SiblingsSpouses  ParentsChildren     Fare  \\\n",
       "0         0            3   1  22.0                1                0   7.2500   \n",
       "1         1            1   0  38.0                1                0  71.2833   \n",
       "2         1            3   0  26.0                0                0   7.9250   \n",
       "3         1            1   0  35.0                1                0  53.1000   \n",
       "4         0            3   1  35.0                0                0   8.0500   \n",
       "\n",
       "  Cherbourg Queenstown Southampton  \n",
       "0         0          0           1  \n",
       "1         1          0           0  \n",
       "2         0          0           1  \n",
       "3         0          0           1  \n",
       "4         0          0           1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training set\n",
    "train['Sex'][train['Sex'] == 'male'] = 1\n",
    "train['Sex'][train['Sex'] == 'female'] = 0\n",
    "cherb = train['Embarked'].copy()\n",
    "cherb[cherb == 'C'] = 1\n",
    "cherb[cherb != 1] = 0\n",
    "queens = train['Embarked'].copy()\n",
    "queens[queens == 'Q'] = 1\n",
    "queens[queens != 1] = 0\n",
    "south = train['Embarked'].copy()\n",
    "south[south == 'S'] = 1\n",
    "south[south != 1] = 0\n",
    "train = train.drop(['Embarked'], axis=1)\n",
    "train['Cherbourg'] = cherb\n",
    "train['Queenstown'] = queens\n",
    "train['Southampton'] = south\n",
    "\n",
    "# Test set\n",
    "test['Sex'][test['Sex'] == 'male'] = 1\n",
    "test['Sex'][test['Sex'] == 'female'] = 0\n",
    "cherb = test['Embarked'].copy()\n",
    "cherb[cherb == 'C'] = 1\n",
    "cherb[cherb != 1] = 0\n",
    "queens = test['Embarked'].copy()\n",
    "queens[queens == 'Q'] = 1\n",
    "queens[queens != 1] = 0\n",
    "south = test['Embarked'].copy()\n",
    "south[south == 'S'] = 1\n",
    "south[south != 1] = 0\n",
    "test = test.drop(['Embarked'], axis=1)\n",
    "test['Cherbourg'] = cherb\n",
    "test['Queenstown'] = queens\n",
    "test['Southampton'] = south\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with missing values comes next. I'll have to find columns with NaN's and decide how to handle these missing values. First, let's see which one of these has missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data -  Survived :  False\n",
      "Train Data -  TicketClass :  False\n",
      "Train Data -  Sex :  False\n",
      "Train Data -  Age :  True\n",
      "Train Data -  SiblingsSpouses :  False\n",
      "Train Data -  ParentsChildren :  False\n",
      "Train Data -  Fare :  False\n",
      "Train Data -  Cherbourg :  False\n",
      "Train Data -  Queenstown :  False\n",
      "Train Data -  Southampton :  False\n",
      "Test Data -  TicketClass :  False\n",
      "Test Data -  Sex :  False\n",
      "Test Data -  Age :  True\n",
      "Test Data -  SiblingsSpouses :  False\n",
      "Test Data -  ParentsChildren :  False\n",
      "Test Data -  Fare :  True\n",
      "Test Data -  Cherbourg :  False\n",
      "Test Data -  Queenstown :  False\n",
      "Test Data -  Southampton :  False\n"
     ]
    }
   ],
   "source": [
    "for col in train.columns:\n",
    "    print(\"Train Data - \", col, \": \", train[col].isnull().values.any())\n",
    "for col in test.columns:\n",
    "    print(\"Test Data - \", col, \": \", test[col].isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Age has missing values for training data, while test has Age and Fare with NaN's. I'll treat both by replacing them with the mean values since the number of samples is already small and we can't afford simply dropping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = train['Age'].mean()\n",
    "train['Age'][train['Age'].isnull()] = avg\n",
    "avg = test['Age'].mean()\n",
    "test['Age'][test['Age'].isnull()] = avg\n",
    "avg = test['Fare'].mean()\n",
    "test['Fare'][test['Fare'].isnull()] = avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Training will be done using cross validation on the training set. For now, that will be 15% of the data. To facilitate easy replication of results, I'll also set a random_state whenever necessary. I'll then decide on the model with the highest validation score to generate test predictions from the testing set. These predictions will then be submitted on Kaggle's site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data\n",
    "For the sake of consistency, I'll generate validation indices here and use the same one for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train.drop(['Survived'], axis=1),\n",
    "                                                      train['Survived'],\n",
    "                                                      test_size=0.15,\n",
    "                                                      stratify=train['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "valid_X and valid_Y (not to be confused with the test data) will be the data used to score each generated model. These will be the same throughout all testing, which should help allow for some measure of consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "I'll start with decision trees and go from there. That way, I can rebuild the tree without Embarked if the initial one gives a low score on its information gain. To find the best hyperparameters, I'll also do a rudimentary form of a grid search by looping over some values for each parameter then keeping the best scoring model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embarked or no Embarked?\n",
    "A quick check to see if Embarked should be included or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10997486, 0.29505985, 0.28100758, 0.04339923, 0.01633061,\n",
       "       0.23128225, 0.00659903, 0.00386169, 0.01248489])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state=5)\n",
    "dt.fit(train_X, train_Y)\n",
    "dt.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three of the embarkation columns have around 1% importance. I think it's safe to say we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.drop(['Cherbourg','Queenstown','Southampton'], axis=1)\n",
    "valid_X = valid_X.drop(['Cherbourg','Queenstown','Southampton'], axis=1)\n",
    "test = test.drop(['Cherbourg','Queenstown','Southampton'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.835820895522388,\n",
       " {'class_weight': None,\n",
       "  'criterion': 'gini',\n",
       "  'max_depth': None,\n",
       "  'max_features': None,\n",
       "  'max_leaf_nodes': None,\n",
       "  'min_impurity_decrease': 0.0,\n",
       "  'min_impurity_split': None,\n",
       "  'min_samples_leaf': 5,\n",
       "  'min_samples_split': 2,\n",
       "  'min_weight_fraction_leaf': 0.0,\n",
       "  'presort': False,\n",
       "  'random_state': 5,\n",
       "  'splitter': 'best'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_best_model = DecisionTreeClassifier()\n",
    "dt_best_score = 0\n",
    "leafs = [1, 2, 5, 10]\n",
    "crit = ['gini', 'entropy']\n",
    "split = ['best']\n",
    "for l in leafs:\n",
    "    for c in crit:\n",
    "        for s in split:\n",
    "            dt = DecisionTreeClassifier(min_samples_leaf=l, \n",
    "                                        criterion=c,\n",
    "                                        splitter=s,\n",
    "                                        random_state=5)\n",
    "            dt.fit(train_X, train_Y)\n",
    "            score = dt.score(valid_X, valid_Y)\n",
    "            if score > dt_best_score:\n",
    "                dt_best_model = dt\n",
    "                dt_best_score = score\n",
    "dt_best_score, dt_best_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing model used criterion='gini', splitter='best' and min_samples_leaf=10. This means that the Gini Index does better than information gain and having 10 points per leaf produces a stronger model. That said, I believe the scores are pretty close for some combinations, especially when tweaking min_samples_leaf but I'll keep this model for argument's sake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at other learners and see if they do any better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network\n",
    "This one has a bunch of parameters to look at: hidden_layer_sizes, activation, solver, alpha, momentum, learning_rate_init are all worth tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8507462686567164,\n",
       " {'activation': 'tanh',\n",
       "  'alpha': 0.1,\n",
       "  'batch_size': 'auto',\n",
       "  'beta_1': 0.9,\n",
       "  'beta_2': 0.999,\n",
       "  'early_stopping': False,\n",
       "  'epsilon': 1e-08,\n",
       "  'hidden_layer_sizes': (6,),\n",
       "  'learning_rate': 'constant',\n",
       "  'learning_rate_init': 0.009000000000000001,\n",
       "  'max_iter': 5000,\n",
       "  'momentum': 0.1,\n",
       "  'nesterovs_momentum': True,\n",
       "  'power_t': 0.5,\n",
       "  'random_state': 5,\n",
       "  'shuffle': True,\n",
       "  'solver': 'adam',\n",
       "  'tol': 0.0001,\n",
       "  'validation_fraction': 0.1,\n",
       "  'verbose': False,\n",
       "  'warm_start': False})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_best_model = MLPClassifier()\n",
    "nn_best_score = 0\n",
    "layers = [(l,)*size for size in [1,2,3] for l in [3,6,9,12]]\n",
    "act = ['tanh','relu']\n",
    "slv = ['sgd','adam']\n",
    "alp = [10**a for a in range(-4,0)]\n",
    "mom = [0.1, 0.5, 1]\n",
    "lrate = [k*10**a for k in [5,9] for a in range(-4,0)]\n",
    "for l in layers:\n",
    "    for ac in act:\n",
    "        for s in slv:\n",
    "            for al in alp:\n",
    "                for m in mom:\n",
    "                    for lr in lrate:\n",
    "                        nn = MLPClassifier(hidden_layer_sizes=l,\n",
    "                                           activation=ac,\n",
    "                                           solver=s,\n",
    "                                           alpha=al,\n",
    "                                           momentum=m,\n",
    "                                           learning_rate_init=lr,\n",
    "                                           max_iter=5000,\n",
    "                                           random_state=5)\n",
    "                        nn.fit(train_X, train_Y)\n",
    "                        score = nn.score(valid_X, valid_Y)\n",
    "                        if score > nn_best_score:\n",
    "                            nn_best_model = nn\n",
    "                            nn_best_score = score\n",
    "nn_best_score, nn_best_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the neural network model slightly outperformed the decision tree. Considering neural nets are all the buzz nowadays, that's nice to see it living up to the hype even though it isn't necessarily deep learning. What's interesting to note is that the hidden layer the for the NN model has a three hidden layers of size 12, which is actually larger than the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "The logit model is up next. I'd expect this to do pretty well considering this is a binary classification problem. Since there aren't many hyperparameters to work with, I'll try both L1 and L2 regularization for the sake of completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8432835820895522,\n",
       " {'C': 1.0,\n",
       "  'class_weight': None,\n",
       "  'dual': False,\n",
       "  'fit_intercept': True,\n",
       "  'intercept_scaling': 1,\n",
       "  'max_iter': 5000,\n",
       "  'multi_class': 'ovr',\n",
       "  'n_jobs': 1,\n",
       "  'penalty': 'l1',\n",
       "  'random_state': 5,\n",
       "  'solver': 'liblinear',\n",
       "  'tol': 0.0001,\n",
       "  'verbose': 0,\n",
       "  'warm_start': False})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_best_model = LogisticRegression()\n",
    "logit_best_score = 0\n",
    "reg = ['l1', 'l2']\n",
    "slvr = ['liblinear', 'saga']\n",
    "for r in reg:\n",
    "    for s in slvr:\n",
    "        logit = LogisticRegression(penalty=r, \n",
    "                                   solver=s, \n",
    "                                   max_iter=5000, \n",
    "                                   random_state=5)\n",
    "        logit.fit(train_X, train_Y)\n",
    "        score = logit.score(valid_X, valid_Y)\n",
    "        if score > logit_best_score:\n",
    "            logit_best_model = logit\n",
    "            logit_best_score = score\n",
    "logit_best_score, logit_best_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Considering the small training set, I could see the random forest model outperforming the decision tree. But it would be interesting to see if it matches the neural network. Again, there are slightly more parameters to tweak here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8955223880597015,\n",
       " {'bootstrap': True,\n",
       "  'class_weight': None,\n",
       "  'criterion': 'entropy',\n",
       "  'max_depth': None,\n",
       "  'max_features': 3,\n",
       "  'max_leaf_nodes': None,\n",
       "  'min_impurity_decrease': 0.0,\n",
       "  'min_impurity_split': None,\n",
       "  'min_samples_leaf': 2,\n",
       "  'min_samples_split': 2,\n",
       "  'min_weight_fraction_leaf': 0.0,\n",
       "  'n_estimators': 10,\n",
       "  'n_jobs': 1,\n",
       "  'oob_score': False,\n",
       "  'random_state': 5,\n",
       "  'verbose': 0,\n",
       "  'warm_start': False})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_best_model = RandomForestClassifier()\n",
    "rf_best_score = 0\n",
    "estimators = [5, 10, 20]\n",
    "crit = ['entropy', 'gini']\n",
    "leafs = [1, 2, 5, 10]\n",
    "feats = [x for x in range(1, train.shape[1]-3)] + ['auto']\n",
    "for e in estimators:\n",
    "    for c in crit:\n",
    "        for l in leafs:\n",
    "            for f in feats:\n",
    "                rf = RandomForestClassifier(n_estimators=e, \n",
    "                                            criterion=c, \n",
    "                                            min_samples_leaf=l, \n",
    "                                            max_features=f, \n",
    "                                            random_state=5)\n",
    "                rf.fit(train_X, train_Y)\n",
    "                score = rf.score(valid_X, valid_Y)\n",
    "                if score > rf_best_score:\n",
    "                    rf_best_model = rf\n",
    "                    rf_best_score = score\n",
    "rf_best_score, rf_best_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that.. Pretty decent for a random learner. The best parameters are in line with the single tree learner, both performing better with criterion='gini' and min_samples_leaf=10. What sets this apart from the single tree is the bootstrapping of 20 learners to use their average, reducing the odds of overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine\n",
    "The most time-consuming model of the group, but worth considering since we have a binary classification problem. What allows this learner to do well depends on whether the data is linearly separable. My guess is no, because chance and happenstance would have played a big part in a passenger's survival. In other words, the passenger is not guaranteed a survivor if they were X age and was in first class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7761194029850746,\n",
       " {'C': 1.0,\n",
       "  'cache_size': 200,\n",
       "  'class_weight': None,\n",
       "  'coef0': 0.0,\n",
       "  'decision_function_shape': 'ovr',\n",
       "  'degree': 3,\n",
       "  'gamma': 'auto',\n",
       "  'kernel': 'linear',\n",
       "  'max_iter': 10000,\n",
       "  'probability': False,\n",
       "  'random_state': 5,\n",
       "  'shrinking': True,\n",
       "  'tol': 0.001,\n",
       "  'verbose': False})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_best_model = SVC()\n",
    "svm_best_score = 0\n",
    "kern = ['rbf', 'linear', 'sigmoid', 'poly']\n",
    "for k in kern:\n",
    "    svm = SVC(kernel = k, \n",
    "              max_iter=10000, \n",
    "              random_state=5)\n",
    "    svm.fit(train_X, train_Y)\n",
    "    score = svm.score(valid_X, valid_Y)\n",
    "    if score > svm_best_score:\n",
    "        svm_best_model = svm\n",
    "        svm_best_score = score\n",
    "svm_best_score, svm_best_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the SVM does not do very well. This proves my theory that the data is not linearly separable and hence support vector machines struggle to properly classify surviving passengers from the Titanic disaster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors\n",
    "The only lazy learner of the bunch, the kNN model feels like a coin toss to me. It has the potential to do well if the passengers tended to flock with one another, and hence have a higher chance of seeing the same outcome. On the other hand, if this doesn't hold true then the model should do poorly, which is the more likely scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8134328358208955,\n",
       " {'algorithm': 'auto',\n",
       "  'leaf_size': 30,\n",
       "  'metric': 'minkowski',\n",
       "  'metric_params': None,\n",
       "  'n_jobs': 1,\n",
       "  'n_neighbors': 5,\n",
       "  'p': 1,\n",
       "  'weights': 'distance'})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_best_model = KNeighborsClassifier()\n",
    "knn_best_score = 0\n",
    "neighbors = [x for x in range(1,11)]\n",
    "wgts = ['uniform', 'distance']\n",
    "power = [x for x in range(1, 5)]\n",
    "for k in neighbors:\n",
    "    for w in wgts:\n",
    "        for pow in power:\n",
    "            knn = KNeighborsClassifier(n_neighbors=k, \n",
    "                                       weights=w, \n",
    "                                       p=pow)\n",
    "            knn.fit(train_X, train_Y)\n",
    "            score = knn.score(valid_X, valid_Y)\n",
    "            if score > knn_best_score:\n",
    "                knn_best_model = knn\n",
    "                knn_best_score = score\n",
    "knn_best_score, knn_best_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprise surprise.. k-NN suffers the same issues with the SVM model. The randomness of passengers' survival does not help a purely deterministic learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short Disclaimer:\n",
    "Running the entire kernel multiple times actually leads to a toss-up between the random forest or neural network as the best model. But for the sake of this exercise I'll stick to the neural net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model\n",
    "Turns out the neural net does best! 87% accuracy is a rather respectable result. But is that better than random? To do that, I'll have to check the Kappa Statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6760937877689147"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "y1 = nn_best_model.predict(valid_X)\n",
    "y2 = valid_Y\n",
    "cohen_kappa_score(y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That value corresponds to a decently good model. Now to generate a new model with the validation data included for training then submit the predictions on Kaggle. I decided to re-include the validation data because every instance matters with a training size of 891."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.append(valid_X)\n",
    "train_Y = train_Y.append(valid_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission_model = RandomForestClassifier(n_estimators=10, \n",
    "                                          criterion='entropy', \n",
    "                                          min_samples_leaf=2, \n",
    "                                          max_features='auto')\n",
    "submission_model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.Series(submission_model.predict(test))\n",
    "indices = pd.Series([x for x in range(892, 1310)])\n",
    "\n",
    "submission = pd.concat([indices, submission], axis=1)\n",
    "submission.columns = ['PassengerId', 'Survived']\n",
    "submission.set_index('PassengerId')\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got 0.7751 for my submission. Not as good as I'd like. One day I'll come back to this with and see what else I can do. My early suspicions is there needs to be much more feature engineering before training the model. 6 features feels too little for my liking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
